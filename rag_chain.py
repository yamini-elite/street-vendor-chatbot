"""
rag_chain.py â€“ RAG pipeline using OpenAI API via LangChain
Streamlit Cloud compatible with minimal memory footprint
"""

from langchain_openai import ChatOpenAI
from langchain_community.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langdetect import detect, LangDetectException
import os

# Language detection and prompts
LANG_PROMPTS = {
    'en': "English", 'hi': "Hindi", 'mr': "Marathi", 'ta': "Tamil",
    'te': "Telugu", 'kn': "Kannada", 'gu': "Gujarati", 'bn': "Bengali",
    'pa': "Punjabi", 'ml': "Malayalam", 'ur': "Urdu"
}

GREETINGS_LIST = [
    "hi", "hello", "hii", "hey",
    "à¤¨à¤®à¤¸à¥à¤¤à¥‡", "à¤¹à¤¾à¤¯", "à¤¨à¤®à¤¸à¥à¤•à¤¾à¤°", "à®µà®£à®•à¯à®•à®®à¯", "à®¹à®¾à®¯à¯",
    "à°¹à°¾à°¯à±", "à´¹à´¾à´¯àµ", "à´¨à´®à´¸àµà´•à´¾à´°à´‚", "à²¹à²¾à²¯à³", "àª¹àª¾àª¯", "à¦¹à§à¦¯à¦¾à¦²à§‹", "à¨¸à¨¤à¨¿ à¨¸à¨¼à©à¨°à©€ à¨…à¨•à¨¾à¨²"
]

GREETINGS_REPLY = {
    'en': "Hello! ğŸ‘‹ How can I help you today with street vendor digitalization?",
    'hi': "à¤¨à¤®à¤¸à¥à¤¤à¥‡! à¤®à¥ˆà¤‚ à¤†à¤ªà¤•à¥‡ à¤¸à¥à¤Ÿà¥à¤°à¥€à¤Ÿ à¤µà¥‡à¤‚à¤¡à¤° à¤¸à¥‡ à¤œà¥à¤¡à¤¼à¥‡ à¤¸à¤µà¤¾à¤²à¥‹à¤‚ à¤®à¥‡à¤‚ à¤•à¥ˆà¤¸à¥‡ à¤®à¤¦à¤¦ à¤•à¤° à¤¸à¤•à¤¤à¤¾ à¤¹à¥‚à¤?",
    'mr': "à¤¨à¤®à¤¸à¥à¤•à¤¾à¤°! à¤®à¥€ à¤¸à¥à¤Ÿà¥à¤°à¥€à¤Ÿ à¤µà¥‡à¤‚à¤¡à¤° à¤µà¤¿à¤·à¤¯à¥€ à¤†à¤ªà¤²à¥à¤¯à¤¾à¤²à¤¾ à¤•à¤¶à¥€ à¤®à¤¦à¤¤ à¤•à¤°à¥‚ à¤¶à¤•à¤¤à¥‹?",
    'ta': "à®µà®£à®•à¯à®•à®®à¯! à®¤à¯†à®°à¯ à®µà®¿à®¯à®¾à®ªà®¾à®°à®¿à®•à®³à¯ à®¤à¯Šà®Ÿà®°à¯à®ªà®¾à®© à®à®¨à¯à®¤ à®‰à®¤à®µà®¿à®¯à¯à®®à¯ à®•à¯‡à®³à¯à®™à¯à®•à®³à¯.",
    'te': "à°¹à°¾à°¯à±! à°¸à±à°Ÿà±à°°à±€à°Ÿà± à°µà±†à°‚à°¡à°°à± à°¸à°‚à°¬à°‚à°§à°¿à°‚à°šà°¿à°¨ à°®à±€ à°ªà±à°°à°¶à±à°¨à°²à°•à± à°¸à°¹à°¾à°¯à°‚ à°šà±‡à°¸à±à°¤à°¾à°¨à±.",
    'gu': "àª¹àª¾àª¯! àª¸à«àªŸà«àª°à«€àªŸ àªµà«‡àª¨à«àª¡àª° àªªà«àª°àª¶à«àª¨à«‹ àª®àª¾àªŸà«‡ àª¹à«àª‚ àª®àª¦àª¦ àª•àª°à«€ àª¶àª•à«àª‚ àª›à«àª‚.",
    'bn': "à¦¹à§à¦¯à¦¾à¦²à§‹! à¦°à¦¾à¦¸à§à¦¤à¦¾à¦° à¦¬à¦¿à¦•à§à¦°à§‡à¦¤à¦¾ à¦¸à¦‚à¦•à§à¦°à¦¾à¦¨à§à¦¤ à¦¯à§‡à¦•à§‹à¦¨à§‹ à¦ªà§à¦°à¦¶à§à¦¨ à¦•à¦°à§à¦¨à¥¤",
    'pa': "à¨¸à¨¤à¨¿ à¨¸à¨¼à©à¨°à©€ à¨…à¨•à¨¾à¨²! à¨¤à©à¨¸à©€à¨‚ à¨¸à¨Ÿà¨°à©€à¨Ÿ à¨µà©ˆà¨‚à¨¡à¨° à¨¸à©°à¨¬à©°à¨§à©€ à¨•à©‹à¨ˆ à¨¸à¨µà¨¾à¨² à¨ªà©à©±à¨›à©‹à¥¤",
    'ml': "à´¹à´¾à´¯àµ! à´¸àµà´Ÿàµà´°àµ€à´±àµà´±àµ à´µàµ†à´£àµà´Ÿàµ¼ à´¸à´‚à´¬à´¨àµà´§à´¿à´šàµà´šàµà´³àµà´³ à´¨à´¿à´™àµà´™à´³àµà´Ÿàµ† à´šàµ‹à´¦àµà´¯à´™àµà´™à´³à´¿àµ½ à´¸à´¹à´¾à´¯à´¿à´•àµà´•à´¾à´‚à¥¤",
}

def get_openai_api_key():
    """Get OpenAI API key from Streamlit secrets or environment"""
    import streamlit as st
    
    # Try to get from Streamlit secrets first
    try:
        return st.secrets["OPENAI_API_KEY"]
    except:
        # Fallback to environment variable
        return os.getenv("OPENAI_API_KEY")

def init_openai_models():
    """Initialize OpenAI models with API key"""
    api_key = get_openai_api_key()
    
    if not api_key:
        return None, None
    
    # Initialize embeddings
    embeddings = OpenAIEmbeddings(
        model="text-embedding-3-small",  # Cost-effective embedding model
        openai_api_key=api_key
    )
    
    # Initialize chat model
    llm = ChatOpenAI(
        model="gpt-3.5-turbo",  # Cost-effective chat model
        temperature=0.7,
        openai_api_key=api_key
    )
    
    return embeddings, llm

# â”€â”€â”€ Initialize models â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
embeddings, llm = init_openai_models()

# â”€â”€â”€ Vector DB â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    if embeddings:
        vectordb = Chroma(
            persist_directory="chroma_db",
            embedding_function=embeddings
        )
        print("âœ… Vector database loaded successfully")
    else:
        vectordb = None
        print("âš ï¸ OpenAI API key not found - vector DB unavailable")
except Exception as e:
    print(f"Warning: Could not load vector database: {e}")
    vectordb = None

# â”€â”€â”€ Memory and Chain â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if llm:
    memory = ConversationBufferMemory(
        return_messages=True,
        input_key="question",
        output_key="answer",
        memory_key="chat_history"
    )
    
    if vectordb:
        base_rag_chain = ConversationalRetrievalChain.from_llm(
            llm=llm,
            retriever=vectordb.as_retriever(search_kwargs={"k": 4}),
            memory=memory
        )
    else:
        base_rag_chain = None
else:
    base_rag_chain = None

def detect_user_language(text):
    """Detect language of user input"""
    try:
        return detect(text)
    except LangDetectException:
        return "en"

def get_greeting_reply(lang_code):
    """Get localized greeting response"""
    return GREETINGS_REPLY.get(lang_code, GREETINGS_REPLY['en'])

def rag_chain(question, forced_language=None):
    """Main RAG function with OpenAI API"""
    
    # Check if OpenAI is available
    if not llm:
        return {"answer": "âš ï¸ Please add your OpenAI API key to use the chatbot. Go to 'Manage app' â†’ 'Settings' â†’ 'Secrets' and add: OPENAI_API_KEY='your-key-here'"}
    
    # Detect or use forced language
    user_lang = forced_language or detect_user_language(question)
    
    # Handle greetings
    question_clean = question.strip().lower()
    if any(greeting in question_clean for greeting in GREETINGS_LIST):
        return {"answer": get_greeting_reply(user_lang)}
    
    # Use RAG chain if available, otherwise direct LLM
    if base_rag_chain:
        try:
            # Create system prompt for language
            lang_name = LANG_PROMPTS.get(user_lang, "English")
            enhanced_question = f"Please answer in {lang_name}. Focus on street vendor digitalization, government schemes, and digital payments in India.\n\nQuestion: {question}"
            
            response = base_rag_chain.invoke({"question": enhanced_question})
            return response
        except Exception as e:
            print(f"RAG chain error: {e}")
            # Fallback to direct LLM
    
    # Direct LLM fallback
    try:
        lang_name = LANG_PROMPTS.get(user_lang, "English")
        prompt = f"""You are a helpful assistant for Indian street vendors. Answer in {lang_name}.
        
        Question: {question}
        
        Provide helpful information about street vendor digitalization, government schemes like PM-SVANidhi, digital payments, UPI setup, or related topics for Indian street vendors."""
        
        response = llm.invoke(prompt)
        return {"answer": response.content}
    except Exception as e:
        return {"answer": f"Sorry, I encountered an error: {str(e)}"}

# Export functions for app.py
__all__ = ['rag_chain', 'LANG_PROMPTS', 'detect_user_language']
